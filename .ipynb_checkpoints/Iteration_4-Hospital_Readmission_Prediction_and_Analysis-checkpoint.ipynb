{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c72fb13",
   "metadata": {},
   "source": [
    "# <font color='orange'>Step 2: Data Understanding</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf5c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas DF operation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import array\n",
    "#pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, StringType, DoubleType, ShortType, DecimalType\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import isnull\n",
    "from pyspark.sql.functions import isnan, when, count, col, round\n",
    "from pyspark.sql.functions import mean\n",
    "from pyspark.sql.types import Row\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "\n",
    "# Modeling + Evaluation\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer, OneHotEncoder, StringIndexer\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder \n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import log_loss\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import rank,sum,col\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorSlicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7bcfc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Findspark can add a startup file to the current IPython profile so that the environment vaiables will \n",
    "# # be properly set and pyspark will be imported upon IPython startup\n",
    "# import findspark\n",
    "# findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74fa3eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/11 21:37:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/11 21:37:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Build a sparksession and build a unique app name\n",
    "spark=SparkSession.builder.appName('iteration-4: prediction_hospital_readmission_rate ').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "662dde3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37484e5a",
   "metadata": {},
   "source": [
    "## <font color='grey'> 2.1: Collect Initial Data</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cfac550",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/ubuntu/Iteration_4:HospitalReadmissionPrediction/Uoa-infosys722/Code/dataset_diabetes/diabetic_data.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read the data from csv file into a dataframe\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m diabetic_data\u001b[38;5;241m=\u001b[39m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset_diabetes/diabetic_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:410\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    408\u001b[0m     path \u001b[38;5;241m=\u001b[39m [path]\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/ubuntu/Iteration_4:HospitalReadmissionPrediction/Uoa-infosys722/Code/dataset_diabetes/diabetic_data.csv"
     ]
    }
   ],
   "source": [
    "# Read the data from csv file into a dataframe\n",
    "diabetic_data=spark.read.option(\"header\",\"true\").csv('dataset_diabetes/diabetic_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92e5525",
   "metadata": {},
   "source": [
    "## <font color='grey'> 2.2: Data Description</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a79ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diabetic_data.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55f9485",
   "metadata": {},
   "source": [
    "## <font color='grey'> 2.3: Explore the Data</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e9b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of rows and columns in the data\n",
    "row=diabetic_data.count()\n",
    "cols=len(diabetic_data.columns)\n",
    "print('Total number of columns are - ', cols)\n",
    "print('\\nTotal number of records are - ', row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fb2e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data.select('readmitted').groupBy('readmitted').count().sort(col('count').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import in the relevant types.\n",
    "from pyspark.sql.types import (StructField,StringType,IntegerType,StructType,FloatType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then create a variable with the correct structure.\n",
    "data_schema = [StructField('encounter_id',IntegerType(),True),\n",
    "              StructField('patient_nbr',IntegerType(),True),\n",
    "              StructField('race',StringType(),True),\n",
    "              StructField('gender',StringType(),True),\n",
    "              StructField('age',StringType(),True),\n",
    "              StructField('weight',StringType(),True),\n",
    "              StructField('admission_type_id',IntegerType(),True),\n",
    "              StructField('discharge_disposition_id',IntegerType(),True),\n",
    "              StructField('admission_source_id',IntegerType(),True),\n",
    "              StructField('time_in_hospital',IntegerType(),True),\n",
    "              StructField('payer_code',StringType(),True),\n",
    "              StructField('medical_specialty',StringType(),True),\n",
    "              StructField('num_lab_procedures',IntegerType(),True),\n",
    "              StructField('num_procedures',IntegerType(),True),\n",
    "              StructField('num_medications',IntegerType(),True),\n",
    "              StructField('number_outpatient',IntegerType(),True),\n",
    "              StructField('number_emergency',IntegerType(),True),\n",
    "              StructField('number_inpatient',IntegerType(),True),\n",
    "              StructField('diag_1',StringType(),True),\n",
    "              StructField('diag_2',StringType(),True),\n",
    "              StructField('diag_3',StringType(),True),\n",
    "              StructField('number_diagnoses',IntegerType(),True),\n",
    "              StructField('max_glu_serum',StringType(),True),\n",
    "              StructField('A1Cresult',StringType(),True),\n",
    "              StructField('metformin',StringType(),True),\n",
    "              StructField('repaglinide',StringType(),True),\n",
    "              StructField('nateglinide',StringType(),True),\n",
    "              StructField('chlorpropamide',StringType(),True),\n",
    "              StructField('glimepiride',StringType(),True),\n",
    "              StructField('acetohexamide',StringType(),True),\n",
    "              StructField('glipizide',StringType(),True),\n",
    "              StructField('glyburide',StringType(),True),\n",
    "              StructField('tolbutamide',StringType(),True),\n",
    "              StructField('pioglitazone',StringType(),True),\n",
    "              StructField('rosiglitazone',StringType(),True),\n",
    "              StructField('acarbose',StringType(),True),\n",
    "              StructField('miglitol',StringType(),True),\n",
    "              StructField('troglitazone',StringType(),True),\n",
    "              StructField('tolazamide',StringType(),True),\n",
    "              StructField('examide',StringType(),True),\n",
    "              StructField('citoglipton',StringType(),True),\n",
    "              StructField('insulin',StringType(),True),\n",
    "              StructField('glyburide-metformin',StringType(),True),\n",
    "              StructField('glipizide-metformin',StringType(),True),\n",
    "              StructField('glimepiride-pioglitazone',StringType(),True),\n",
    "              StructField('metformin-rosiglitazone',StringType(),True),\n",
    "              StructField('metformin-pioglitazone',StringType(),True),\n",
    "              StructField('change',StringType(),True),\n",
    "              StructField('diabetesMed',StringType(),True),\n",
    "              StructField('readmitted',StringType(),True)]\n",
    "\n",
    "final_struct = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa48d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data=spark.read.option(\"header\",\"true\").csv('dataset_diabetes/diabetic_data.csv',schema=final_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5411b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate two object for continuous and categorical data\n",
    "numeric_columns = [column[0] for column in diabetic_data.dtypes if column[1]=='int']\n",
    "categorical_data=[column[0] for column in diabetic_data.dtypes if column[1]=='string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d2ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n=[]\n",
    "# for col in diabetic_data.dtypes:\n",
    "#     if col[1]=='int':\n",
    "#         n.append(col[0])\n",
    "# n\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41807025",
   "metadata": {},
   "source": [
    "## Pyspark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6ae7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "diabetic_data.createOrReplaceTempView('diabetic_data')\n",
    "\n",
    "# After that, we can use the SQL programming language for queries. \n",
    "results = spark.sql(\"SELECT * FROM diabetic_data\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44ae896",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select distinct race from diabetic_data\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select distinct gender from diabetic_data\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9405903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countplot(df):\n",
    "    for col in categorical_data:\n",
    "        plt.figure(figsize=(15,5))\n",
    "        sns.countplot(data=(diabetic_data.select(col).toPandas()), x=col)\n",
    "        plt.show()\n",
    "countplot(diabetic_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d65798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.countplot(data=(diabetic_data.select('readmitted').toPandas()), x='readmitted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f369b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df3 = diabetic_data.withColumn(\"readmitted\", when(diabetic_data.readmitted == \"NO\",0) \\\n",
    "      .when(diabetic_data.readmitted == \">30\",1).when(diabetic_data.readmitted == \"<30\",2) \\\n",
    "      .otherwise(diabetic_data.readmitted))\n",
    "# change the readmitted data types from string to integar after changing the values in new data frame, just for the \n",
    "#viaualization purpose\n",
    "df3=df3.withColumn(\"readmitted\",df3.readmitted.cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def barplot(df):\n",
    "    for col in numeric_columns:\n",
    "        plt.figure(figsize=(15,5))\n",
    "        sns.barplot(data=(df.toPandas()), y=col,x='readmitted')\n",
    "        plt.show()\n",
    "barplot(diabetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b3c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(df):\n",
    "    for col in numeric_columns:\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.hist(data=(diabetic_data.select(col).toPandas()),x=col,bins=15, color='b')\n",
    "        plt.title(col)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93619c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(diabetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd4bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sns.barplot(data=(diabetic_data.select('number_emergency','readmitted').filter(\"number_emergency<=1\").toPandas()), y='number_emergency',x='readmitted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4c2e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sns.barplot(data=(diabetic_data.select('number_emergency','readmitted').filter(\"number_emergency>1\").toPandas()), y='number_emergency',x='readmitted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3563f8",
   "metadata": {},
   "source": [
    "## <font color='grey'> 2.4: Data Quality</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd03c9f",
   "metadata": {},
   "source": [
    "### Check Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dfbe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using List comprehension, we will check the mising values in the data\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "df1 = diabetic_data.select([count(when(col(c).contains('Nan') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in diabetic_data.columns])\n",
    "df1.show()\n",
    "\n",
    "#looks like data is clean but has missing values with \"?\" sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adffec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many columns has contained \"?\" sign with how many values\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "df2 = diabetic_data.select([count(when(col(c).contains('?') | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in diabetic_data.columns])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd8d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now We will check the missing values in the data\n",
    "# # Using List comprehension, we will check the mising values in the data\n",
    "# df3 = diabetic_data.select([count(when(col(c).contains('Nan') | \\\n",
    "#                             col(c).contains('NULL') | \\\n",
    "#                             (col(c) == '' ) | (col(c) == '?' )| \\\n",
    "#                             col(c).isNull() | \\\n",
    "#                             isnan(c), c \n",
    "#                            )).alias(c)\n",
    "#                     for c in diabetic_data.columns])\n",
    "# df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e0c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # numeric_cols=diabetic_data.\n",
    "# diabetic_data.race.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diabetic_data.select('time_in_hospital').approxQuantile(probabilities=[0.25],relativeError=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af44348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q1=diabetic_data.approxQuantile('time_in_hospital',[0.25],relativeError=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR=q3[0]-q1[0]\n",
    "# IQR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d095e",
   "metadata": {},
   "source": [
    "## <font color='grey'> 2.4: Data Quality</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4603ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as f\n",
    "# def find_outliers(df):\n",
    "\n",
    "#     # Identifying the numerical columns in a spark dataframe\n",
    "#     numeric_columns = [column[0] for column in df.dtypes if column[1]=='int']\n",
    "\n",
    "#     # Using the `for` loop to create new columns by identifying the outliers for each feature\n",
    "#     for column in numeric_columns:\n",
    "\n",
    "#         less_Q1 = 'less_Q1_{}'.format(column)\n",
    "#         more_Q3 = 'more_Q3_{}'.format(column)\n",
    "#         Q1 = 'Q1_{}'.format(column)\n",
    "#         Q3 = 'Q3_{}'.format(column)\n",
    "\n",
    "#         # Q1 : First Quartile ., Q3 : Third Quartile\n",
    "#         Q1 = df.approxQuantile(column,[0.25],relativeError=0)\n",
    "#         Q3 = df.approxQuantile(column,[0.75],relativeError=0)\n",
    "        \n",
    "#         # IQR : Inter Quantile Range\n",
    "#         # We need to define the index [0], as Q1 & Q3 are a set of lists., to perform a mathematical operation\n",
    "#         # Q1 & Q3 are defined seperately so as to have a clear indication on First Quantile & 3rd Quantile\n",
    "#         IQR = Q3[0] - Q1[0]\n",
    "        \n",
    "#         #selecting the data, with -1.5*IQR to + 1.5*IQR., where param = 1.5 default value\n",
    "#         less_Q1 =  Q1[0] - 1.5*IQR\n",
    "#         more_Q3 =  Q3[0] + 1.5*IQR\n",
    "        \n",
    "#         isOutlierCol = 'is_outlier_{}'.format(column)\n",
    "        \n",
    "#         df_1=df.filter((df[col]<=less_Q1) | (df[col]>=more_Q3) )\n",
    "# #         df_2=df.filter(df[col]>=more_Q3)\n",
    "# # #         df_1=df[(df.select(col)<=lower)]\n",
    "# # #         df_2=df[(df.select(col)>=upper)]\n",
    "# #         df=unionAll([df_1,df_2])\n",
    "# #         df = df.withColumn(isOutlierCol,f.when((df[column] > more_Q3) | (df[column] < less_Q1), 1).otherwise(0))\n",
    "    \n",
    "\n",
    "# #     # Selecting the specific columns which we have added above, to check if there are any outliers\n",
    "# #     selected_columns = [column for column in df.columns if column.startswith(\"is_outlier\")]\n",
    "\n",
    "# #     # Adding all the outlier columns into a new colum \"total_outliers\", to see the total number of outliers\n",
    "# #     df = df.withColumn('total_outliers',sum(df[column] for column in selected_columns))\n",
    "\n",
    "# #     # Dropping the extra columns created above, just to create nice dataframe., without extra columns\n",
    "# #     df = df.drop(*[column for column in df.columns if column.startswith(\"is_outlier\")])\n",
    "\n",
    "#     return df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ae7a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataframe=diabetic_data.select('total_outliers')\n",
    "# x=find_outliers(diabetic_data)\n",
    "# x.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09461d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functools\n",
    " \n",
    "# # explicit function\n",
    "# def unionAll(dfs):\n",
    "#     return functools.reduce(lambda df1, df2: df1.union(df2.select(df1.columns)), dfs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a7efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Outlier using IQR\n",
    "def finding_outlier(df,col):\n",
    "#     for col in numeric_columns:\n",
    "    df.select(col).toPandas().astype(int)\n",
    "    q1=df.approxQuantile(col,[0.25],relativeError=0)\n",
    "    q3=df.approxQuantile(col,[0.75],relativeError=0)\n",
    "    IQR=q3[0]-q1[0]\n",
    "    upper=q3[0]+1.5*IQR\n",
    "    lower=q1[0]-1.5*IQR\n",
    "    df_1=df.filter((df[col]<=lower) | (df[col]>=upper) )\n",
    "#         df_1=df[(df.select(col)<=lower)]\n",
    "#         df_2=df[(df.select(col)>=upper)]\n",
    "#     df=unionAll([df_1,df_2])\n",
    "\n",
    "    return df_1.select(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb32eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "finding_outlier(diabetic_data,'num_medications').groupby('num_medications').count().withColumnRenamed('count','outliers').sort(('num_medications')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886b90a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding_outlier(diabetic_data,'num_medications').groupby('num_medications').count().withColumnRenamed('count','outliers').sort(('num_medications')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80268e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_count():\n",
    "    for col in numeric_columns:\n",
    "        print(f\"{col}\",finding_outlier(diabetic_data,col).count())\n",
    "outlier_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a387ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checked the statistics\n",
    "diabetic_data.toPandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d1f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=(diabetic_data.select('num_medications').toPandas()),x='num_medications')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9b78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot(df):\n",
    "    for col in numeric_columns:\n",
    "        plt.figure(figsize=(15,5))\n",
    "        sns.boxplot(data=(diabetic_data.select(col).toPandas()),x=col)\n",
    "        plt.show()\n",
    "boxplot(diabetic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d287f6",
   "metadata": {},
   "source": [
    "# <font color='orange'>Step 3: Data Preparation</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5d59a7",
   "metadata": {},
   "source": [
    "## <font color='grey'> 3.1: Selecting the Data </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f2d6dc",
   "metadata": {},
   "source": [
    "### <font color='blue'> 3.1.1: Selecting items(rows) </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data.filter((diabetic_data.readmitted=='<30')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed594cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data.select('gender').toPandas().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0137a8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data.filter((diabetic_data.gender==\"Unknown/Invalid\") & (diabetic_data.readmitted=='<30')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78afecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data=diabetic_data.filter(diabetic_data.gender!='Unknown/Invalid')\n",
    "diabetic_data.select('gender').toPandas().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d769f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data.select('race').toPandas().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ea030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Filter Based on List Values\n",
    "# li=['Caucasian','AfricanAmerican','Hispanic','Other','Asian']\n",
    "# diabetic_data.filter((diabetic_data.race.isin(li)==False)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.filter(df.state.isin(li)==False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0dc82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_stroke[df_stroke['bmi'].isna() & df_stroke['stroke']==1]['stroke'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72831de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total 188 values of \"?\" in race column are contributing towards the target variable\n",
    "diabetic_data.filter((diabetic_data.race==\"?\") & (diabetic_data.readmitted=='<30')).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97716490",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data=diabetic_data.filter(diabetic_data.race!='?')\n",
    "diabetic_data.select('race').toPandas().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd5142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"update diabetic_data set readmitted=0 where readmitted='NO' \").show()\n",
    "# spark.sql(\"select readmitted from diabetic_data\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"update diabetic_data set readmitted=0 where readmitted='>30' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1293bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the readmitted column \n",
    "diabetic_data=diabetic_data.withColumn(\"readmitted\", when(col(\"readmitted\") == \"NO\",0)\n",
    "      .when(col(\"readmitted\") == \"<30\",1).when(col(\"readmitted\") == \">30\",0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data.select('readmitted').groupBy('readmitted').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a703b53f",
   "metadata": {},
   "source": [
    "### <font color='blue'> 3.1.2: Selecting Attributes(Columns) </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd70a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encounter_id & patient_nbr are just for the records purpose only. So I am deleting these attributes\n",
    "# diag_1, diag_2, diag_3 have almost similar values. Hence these are correlated,we will use only the diag_1 feature.\n",
    "diabetic_data=diabetic_data.drop(\"encounter_id\",\"patient_nbr\",\"diag_2\",\"diag_3\")\n",
    "diabetic_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612fda84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of rows and columns in the data after removing some attributes and records\n",
    "row=diabetic_data.count()\n",
    "cols=len(diabetic_data.columns)\n",
    "print('Total number of columns are - ', cols)\n",
    "print('\\nTotal number of records are - ', row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1919f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the numeric columns now\n",
    "numeric_columns = [column[0] for column in diabetic_data.dtypes if column[1]=='int']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381ae0fc",
   "metadata": {},
   "source": [
    "## <font color='grey'> 3.2: Clean the Data</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70413eb",
   "metadata": {},
   "source": [
    "### <font color='blue'> 3.2.1: Clean Missing Values </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9178112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "As We can see in step 2.3, there is no missing values in the data, but I have found the missing values are \n",
    "imputed with \"?\" sign. So, First of all we will replace the \"?\" sign with none value. and then we will check\n",
    "the missing values\n",
    "\"\"\"\n",
    "print(\" IMPORTANT 👆👆\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"?\" with 'nan' values\n",
    "diabetic_data = diabetic_data.replace({'?':''}, subset=['race','weight','payer_code','medical_specialty','diag_1','diag_2','diag_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d8047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Check is there any values left with \"?\"\n",
    "diabetic_data.select([count(when(col(c).contains('?') | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in diabetic_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10354b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# diabetic_data.select([count(when((col(c)=='') | \\\n",
    "#                             isnan(c), c \n",
    "#                            )).alias(c)\n",
    "#                     for c in diabetic_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc65f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check the missing values in the dataset \n",
    "diabetic_data.select([count(when(col(c).contains('NaN') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in diabetic_data.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(round((diabetic_data.filter(diabetic_data.weight=='').count()*100)/(diabetic_data.select('weight').count()),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b8fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "# for cols in diabetic_data.columns:\n",
    "#     print(round((diabetic_data.filter(\" cols =='' \").count()*100)\\\n",
    "#                 /(diabetic_data.select(cols).count()),2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column weight, payer code, & Medical speciality has Approximately more than 40% missing records\n",
    "# I will drop these columns from the dataset\n",
    "diabetic_data=diabetic_data.drop('weight','payer_code','medical_specialty')\n",
    "# diabetic_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ce6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the missing values in the data\n",
    "diabetic_data=diabetic_data.filter(diabetic_data.diag_1!='')\n",
    "# diabetic_data=diabetic_data.filter(diabetic_data.diag_2!='')\n",
    "# diabetic_data=diabetic_data.filter(diabetic_data.diag_3!='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8afcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check again the missing values in the dataset \n",
    "diabetic_data.select([count(when(col(c).contains('NaN') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in diabetic_data.columns]).show()\n",
    "\n",
    "# Now there is no missing data in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeff387",
   "metadata": {},
   "source": [
    "### <font color='blue'> 3.2.2: Clean Outliers </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147ff593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After cleaning the missing value, let's find out the total number of outliers\n",
    "# I have made a function to count the outliers. let's run this\n",
    "outlier_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7f3170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Finding Outlier using IQR\n",
    "# def finding_outlier(df,col):\n",
    "# #     for col in numeric_columns:\n",
    "#     df.select(col).toPandas().astype(int)\n",
    "#     q1=df.approxQuantile(col,[0.25],relativeError=0)\n",
    "#     q3=df.approxQuantile(col,[0.75],relativeError=0)\n",
    "#     IQR=q3[0]-q1[0]\n",
    "#     upper=q3[0]+1.5*IQR\n",
    "#     lower=q1[0]-1.5*IQR\n",
    "#     df_1=df.filter((df[col]<=lower) | (df[col]>=upper) )\n",
    "# #         df_1=df[(df.select(col)<=lower)]\n",
    "# #         df_2=df[(df.select(col)>=upper)]\n",
    "# #     df=unionAll([df_1,df_2])\n",
    "\n",
    "#     return df_1.select(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data.toPandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccdb00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_removed(df,colm):\n",
    "#     df.select(col).toPandas().astype(int)\n",
    "    q1=df.approxQuantile(colm,[0.25],relativeError=0)\n",
    "    q3=df.approxQuantile(colm,[0.75],relativeError=0)\n",
    "    IQR=q3[0]-q1[0]\n",
    "    upper=q3[0]+1.5*IQR\n",
    "    lower=q1[0]-1.5*IQR\n",
    "#     df=df[(df[colm]>lower) & (df[colm]<upper)]\n",
    "#     df.filter((df[colm]>lower) & (df[colm]<upper) )\n",
    "    return df.filter((df[colm]>lower) & (df[colm]<upper) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99309019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diabetic_data=\n",
    "diabetic_data=outliers_removed(diabetic_data,'admission_type_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c8c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data=outliers_removed(diabetic_data,'discharge_disposition_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f594e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data=outliers_removed(diabetic_data,'admission_source_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data=outliers_removed(diabetic_data,'time_in_hospital')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26080b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data=outliers_removed(diabetic_data,'num_lab_procedures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d67925",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data=outliers_removed(diabetic_data,'num_procedures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f83cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data=outliers_removed(diabetic_data,'num_medications')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c44b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data=outliers_removed(diabetic_data,'number_diagnoses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a63cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data.toPandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a89e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diabetic_data=\n",
    "diabetic_data=outliers_removed(diabetic_data,'admission_type_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dbddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous=['admission_type_id',\n",
    " 'discharge_disposition_id',\n",
    " 'admission_source_id',\n",
    " 'time_in_hospital',\n",
    " 'num_lab_procedures',\n",
    " 'num_procedures',\n",
    " 'num_medications',\n",
    " 'number_diagnoses']\n",
    "\n",
    "for cols in continuous:\n",
    "    plt.figure(figsize=(15,5))\n",
    "    sns.boxplot(data=(diabetic_data.select(cols).toPandas()),x=cols)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e285910",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data.toPandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d25208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of rows and columns in the data after Cleaning the data\n",
    "row=diabetic_data.count()\n",
    "cols=len(diabetic_data.columns)\n",
    "print('Total number of columns are - ', cols)\n",
    "print('\\nTotal number of records are - ', row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913ef7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in continuous:\n",
    "    plt.figure(figsize=(15,5))\n",
    "    sns.displot(data=(diabetic_data.select(cols).toPandas()),x=cols)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will plot a boxplot of different variable with respect of target attribute stroke\n",
    "for cols in continuous:\n",
    "    plt.figure(figsize=(10,8))\n",
    "\n",
    "    sns.boxplot(x='readmitted',y=cols,hue='readmitted', data=(diabetic_data.toPandas()))\n",
    "    plt.legend(loc='upper left',title='Stroke')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5141f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a62855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a4cf79e",
   "metadata": {},
   "source": [
    "## <font color='grey'> 3.3: Constructing New Data</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec74fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=(diabetic_data.select('age').toPandas()),x='age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3597893",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data.select('age').groupBy('age').count().sort('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a857b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column based on the the previous column\n",
    "diabetic_data=diabetic_data.withColumn(\"age_category\",col(\"age\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b7797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the values in the reconstructing column\n",
    "diabetic_data=diabetic_data.replace({'[0-10)':'Minor','[10-20)':'Minor','[20-30)':'Young_adult',\n",
    "               '[30-40)':'Young_adult', '[40-50)':'Middle_aged','[50-60)':'Middle_aged',\n",
    "               '[60-70)':'Older_adult','[70-80)':'Older_adult',\n",
    "               '[80-90)':'Elderly','[90-100)':'Elderly'},subset=['age_category'])\n",
    "# Check the total values in each category\n",
    "diabetic_data.select('age_category').groupBy('age_category').count().sort('age_category').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d9add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THe comparision of the previous and the current \n",
    "sns.countplot(data=(diabetic_data.select('age','readmitted').toPandas()),x='age',hue='readmitted')\n",
    "plt.show()\n",
    "sns.countplot(data=(diabetic_data.select('age_category','readmitted').toPandas()),x='age_category',hue='readmitted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c8fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the previous column after reconstructing the new column\n",
    "diabetic_data=diabetic_data.drop('age')\n",
    "diabetic_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb569a",
   "metadata": {},
   "source": [
    "## <font color='grey'> 3.4: Integrating Data</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d5b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.cache() is advised for consistency, because without it limited_df and rest_df can have overlapping rows\n",
    "limited_df = diabetic_data.limit(25000).cache()\n",
    "rest_df = diabetic_data.subtract(limited_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5234cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(limited_df.count())\n",
    "# limited_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0957b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rest_df.count())\n",
    "# rest_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c2ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    " \n",
    "# explicit function\n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1, df2: df1.union(df2.select(df1.columns)), dfs)\n",
    " \n",
    " \n",
    "diabetic_data = unionAll([limited_df,rest_df])\n",
    "diabetic_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ec3d75",
   "metadata": {},
   "source": [
    "## <font color='grey'> 3.5: Data Formatting</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a64a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3dcd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast Course_Fees from integer type to float type\n",
    "from pyspark.sql.types import BooleanType\n",
    "diabetic_data = diabetic_data.withColumn(\"readmitted\", \n",
    "                                  diabetic_data[\"readmitted\"]\n",
    "                                  .cast(BooleanType()))\n",
    "diabetic_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55364eb2",
   "metadata": {},
   "source": [
    "# <font color='orange'>Step 4: Data Transformation</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491660b",
   "metadata": {},
   "source": [
    "## <font color='grey'> 4.1: Data Reduction</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e486d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=StringIndexer(inputCol=\"race\", outputCol=\"Race\")\n",
    "b=StringIndexer(inputCol=\"gender\", outputCol=\"Gender\")\n",
    "c=StringIndexer(inputCol=\"max_glu_serum\", outputCol=\"Max_glu_serum\")\n",
    "d=StringIndexer(inputCol=\"A1Cresult\", outputCol=\"AA1Cresult\")\n",
    "e=StringIndexer(inputCol=\"metformin\", outputCol=\"Metformin\")\n",
    "f=StringIndexer(inputCol=\"repaglinide\", outputCol=\"Repaglinide\")\n",
    "g=StringIndexer(inputCol=\"nateglinide\", outputCol=\"Nateglinide\")\n",
    "h=StringIndexer(inputCol=\"chlorpropamide\", outputCol=\"Chlorpropamide\")\n",
    "i=StringIndexer(inputCol=\"glimepiride\", outputCol=\"Glimepiride\")\n",
    "j=StringIndexer(inputCol=\"acetohexamide\", outputCol=\"Acetohexamide\")\n",
    "k=StringIndexer(inputCol=\"glipizide\", outputCol=\"Glipizide\")\n",
    "l=StringIndexer(inputCol=\"glyburide\", outputCol=\"Glyburide\")\n",
    "m=StringIndexer(inputCol=\"tolbutamide\", outputCol=\"Tolbutamide\")\n",
    "n=StringIndexer(inputCol=\"pioglitazone\", outputCol=\"Pioglitazone\")\n",
    "o=StringIndexer(inputCol=\"rosiglitazone\", outputCol=\"Rosiglitazone\")\n",
    "p=StringIndexer(inputCol=\"acarbose\", outputCol=\"Acarbose\")\n",
    "q=StringIndexer(inputCol=\"miglitol\", outputCol=\"Miglitol\")\n",
    "r=StringIndexer(inputCol=\"troglitazone\", outputCol=\"Troglitazone\")\n",
    "s=StringIndexer(inputCol=\"tolazamide\", outputCol=\"Tolazamide\")\n",
    "t=StringIndexer(inputCol=\"examide\", outputCol=\"Examide\")\n",
    "u=StringIndexer(inputCol=\"citoglipton\", outputCol=\"Citoglipton\")\n",
    "v=StringIndexer(inputCol=\"insulin\", outputCol=\"Insulin\")\n",
    "w=StringIndexer(inputCol=\"glyburide-metformin\", outputCol=\"Glyburide_metformin\")\n",
    "x=StringIndexer(inputCol=\"glipizide-metformin\", outputCol=\"Glipizide_metformin\")\n",
    "y=StringIndexer(inputCol=\"glimepiride-pioglitazone\", outputCol=\"Glimepiride_pioglitazone\")\n",
    "z=StringIndexer(inputCol=\"metformin-rosiglitazone\", outputCol=\"Metformin_rosiglitazone\")\n",
    "a1=StringIndexer(inputCol=\"metformin-pioglitazone\", outputCol=\"Metformin_pioglitazone\")\n",
    "a2=StringIndexer(inputCol=\"change\", outputCol=\"Change\")\n",
    "a3=StringIndexer(inputCol=\"diabetesMed\", outputCol=\"DiabetesMed\")\n",
    "a4=StringIndexer(inputCol=\"age_category\", outputCol=\"Age_category\")\n",
    "a5=StringIndexer(inputCol=\"diag_1\", outputCol=\"Diag_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad65796",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc=diabetic_data\n",
    "aa=a.fit(abc).transform(abc)\n",
    "ab=b.fit(aa).transform(aa)\n",
    "ac=c.fit(ab).transform(ab)\n",
    "ad=d.fit(ac).transform(ac)\n",
    "ae=e.fit(ad).transform(ad)\n",
    "af=f.fit(ae).transform(ae)\n",
    "ag=g.fit(af).transform(af)\n",
    "ah=h.fit(ag).transform(ag)\n",
    "ai=i.fit(ah).transform(ah)\n",
    "aj=j.fit(ai).transform(ai)\n",
    "ak=k.fit(aj).transform(aj)\n",
    "al=l.fit(ak).transform(ak)\n",
    "am=m.fit(al).transform(al)\n",
    "an=n.fit(am).transform(am)\n",
    "ao=o.fit(an).transform(an)\n",
    "ap=p.fit(ao).transform(ao)\n",
    "aq=q.fit(ap).transform(ap)\n",
    "ar=r.fit(aq).transform(aq)\n",
    "ast=s.fit(ar).transform(ar)\n",
    "at=t.fit(ast).transform(ast)\n",
    "au=u.fit(at).transform(at)\n",
    "av=v.fit(au).transform(au)\n",
    "aw=w.fit(av).transform(av)\n",
    "ax=x.fit(aw).transform(aw)\n",
    "ay=y.fit(ax).transform(ax)\n",
    "az=a1.fit(ay).transform(ay)\n",
    "ba=a2.fit(az).transform(az)\n",
    "bb=a3.fit(ba).transform(ba)\n",
    "bc=a4.fit(bb).transform(bb)\n",
    "bd=a5.fit(bc).transform(bc)\n",
    "# be=a.fit(bd).transform(bd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bd.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888ce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcd=bd.select('Race', 'Gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'Diag_1', 'number_diagnoses', 'Max_glu_serum', 'A1Cresult', 'Metformin', 'Repaglinide', 'Nateglinide', 'Chlorpropamide', 'Glimepiride', 'Acetohexamide', 'Glipizide', 'Glyburide', 'Tolbutamide', 'Pioglitazone', 'Rosiglitazone', 'Acarbose', 'Miglitol', 'Troglitazone', 'Tolazamide', 'Examide', 'Citoglipton', 'Insulin', 'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone', 'Change', 'DiabetesMed',  'Age_category', 'AA1Cresult', 'Glyburide_metformin', 'Glipizide_metformin', 'Glimepiride_pioglitazone', 'Metformin_pioglitazone','readmitted')\n",
    "len(bcd.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f4086",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcd=bcd.drop( 'A1Cresult',  'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone')\n",
    "bcd.columns\n",
    "len(bcd.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a61b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data=bcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a52d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data.select('citoglipton').toPandas().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data.select('examide').toPandas().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column examide and 'citrogilptone' has only 1 values, so to avoid biaseness, we are removing these columns\n",
    "diabetic_data=diabetic_data.drop('examide','citoglipton')\n",
    "len(diabetic_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e874de9",
   "metadata": {},
   "source": [
    "## <font color='grey'> 4.2: Data Projection</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a0088",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_col=diabetic_data.columns\n",
    "n_col.remove(\"readmitted\")\n",
    "\n",
    "# Let us import the vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=n_col,outputCol=\"features\")\n",
    "\n",
    "# Now let us use the transform method to transform our dataset\n",
    "diabetic_data_1=assembler.transform(diabetic_data)\n",
    "diabetic_data_1.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a31d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "standardscaler=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "diabetic_data_1=standardscaler.fit(diabetic_data_1).transform(diabetic_data_1)\n",
    "diabetic_data_1.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b64893",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_data_1 = diabetic_data_1.withColumn(\"readmitted\", \n",
    "                                  diabetic_data_1[\"readmitted\"]\n",
    "                                  .cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ea44fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size=float(diabetic_data_1.select(\"readmitted\").count())\n",
    "numPositives=diabetic_data_1.select(\"readmitted\").where('readmitted == 1').count()\n",
    "per_ones=(float(numPositives)/float(dataset_size))*100\n",
    "numNegatives=float(dataset_size-numPositives)\n",
    "print('The number of ones are {}'.format(numPositives))\n",
    "print('Percentage of ones are {}'.format(per_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d658b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implement oversampling method\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# calculate ratio\n",
    "major_df = diabetic_data_1.filter(diabetic_data_1.readmitted == 0)\n",
    "minor_df = diabetic_data_1.filter(diabetic_data_1.readmitted == 1)\n",
    "ratio = int(major_df.count()/minor_df.count())\n",
    "print(\"ratio: {}\".format(ratio))\n",
    "a = range(ratio)\n",
    "\n",
    "# duplicate the minority rows\n",
    "oversampled_df = minor_df.withColumn(\"dummy\", F.explode(F.array([F.lit(x) for x in a]))).drop('dummy')\n",
    "\n",
    "# combine both oversampled minority rows and previous majority rows \n",
    "combined_df = major_df.unionAll(oversampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32898af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size=float(combined_df.select(\"readmitted\").count())\n",
    "numPositives=combined_df.select(\"readmitted\").where('readmitted == 1').count()\n",
    "per_ones=(float(numPositives)/float(dataset_size))*100\n",
    "numNegatives=float(dataset_size-numPositives)\n",
    "print('The number of ones are {}'.format(numPositives))\n",
    "print('Percentage of ones are {}'.format(per_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490596cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.select('readmitted').toPandas().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e2b90",
   "metadata": {},
   "source": [
    "# <font color='orange'>Step 5: Data Mining Method</font> \n",
    "\n",
    "5.1 - Match and discuss DM methods within the context of the DM objectives.\n",
    "\n",
    "5.2 - Select the appropriate DM method(s) in a logical manner. The selected DM method must be in line with the data mining goal/success criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba457f6",
   "metadata": {},
   "source": [
    "## <font color='grey'> 5.1: Match and Discuss DM Methods</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa4e84",
   "metadata": {},
   "source": [
    "## <font color='grey'> 5.2: Select Appropriate DM Methods</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a88653",
   "metadata": {},
   "source": [
    "#  <font color='orange'>Step 6: Data Mining Algorithm Selection</font> \n",
    "\n",
    "6.1 Conduct exploratory analysis of DM algorithms within the context of the DM objectives. Then, discuss the analysis.\n",
    "6.2 - Select algorithm(s) in a logical manner based on the exploratory analysis and discussion.\n",
    "6.3 - Model(s) must be selected/built, and the appropriate algorithm/model parameter(s) must be selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e213bb3c",
   "metadata": {},
   "source": [
    "## <font color='grey'> 6.1: Exploratory analysis of DM algorithms</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a027497",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = (combined_df.select('Scaled_features','readmitted')).randomSplit([0.70, 0.30], seed = 12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8848bed9",
   "metadata": {},
   "source": [
    "### <font color='skyblue'> 6.1.1:Logistic Regression Algorithm</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"readmitted\", featuresCol=\"Scaled_features\",maxIter=10)\n",
    "model=lr.fit(train)\n",
    "predict_train=model.transform(train)\n",
    "predict_test=model.transform(test)\n",
    "\n",
    "# from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator=BinaryClassificationEvaluator(rawPredictionCol='rawPrediction',labelCol='readmitted')\n",
    "predict_test.select(\"readmitted\",\"rawPrediction\",\"prediction\",\"probability\").show(20)\n",
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predict_train)))\n",
    "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd37f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion Matrix\n",
    "cm_lr_result = predict_test.crosstab(\"prediction\", \"readmitted\")\n",
    "cm_lr_result = cm_lr_result.toPandas()\n",
    "cm_lr_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate Accuracy, Sensitivity, Specificity, Precision\n",
    "TP = cm_lr_result[\"1\"][0]\n",
    "FP = cm_lr_result[\"0\"][0]\n",
    "TN = cm_lr_result[\"0\"][1]\n",
    "FN = cm_lr_result[\"1\"][1]\n",
    "Accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "Sensitivity = TP/(TP+FN)\n",
    "Specificity = TN/(TN+FP)\n",
    "Precision = TP/(TP+FP)\n",
    "\n",
    "print (\"Accuracy = %0.2f\" %Accuracy )\n",
    "print (\"Sensitivity/Recall = %0.2f\" %Sensitivity )\n",
    "print (\"Specificity = %0.2f\" %Specificity )\n",
    "print (\"Precision = %0.2f\" %Precision )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7962f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model.summary.roc.select('FPR').collect(),\n",
    "         model.summary.roc.select('TPR').collect())\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n",
    "model.summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5bd459",
   "metadata": {},
   "source": [
    "### <font color='skyblue'> 6.1.2:Random Forest Algorithm</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec39cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'Scaled_features', labelCol = \n",
    "                            'readmitted')\n",
    "rf_model = rf.fit(train)\n",
    "predict_train_rf=rf_model.transform(train)\n",
    "predict_test_rf=rf_model.transform(test)\n",
    "predict_test_rf.select(\"readmitted\",\"prediction\").show(10)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "predictions = rf_model.transform(test)\n",
    "auc = BinaryClassificationEvaluator().setLabelCol('readmitted')\n",
    "print('AUC of the model:' + str(auc.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00cccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator=BinaryClassificationEvaluator(rawPredictionCol='rawPrediction',labelCol='readmitted')\n",
    "predict_test.select(\"readmitted\",\"rawPrediction\",\"prediction\",\"probability\").show(20)\n",
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predict_train_rf)))\n",
    "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_test_rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06743bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Grafik\n",
    "PredAndLabels           = predict_test_rf.select(\"probability\", \"readmitted\")\n",
    "PredAndLabels_collect   = PredAndLabels.collect()\n",
    "PredAndLabels_list      = [(float(i[0][0]), 1.0-float(i[1])) for i in PredAndLabels_collect]\n",
    "PredAndLabels           = sc.parallelize(PredAndLabels_list)\n",
    "\n",
    "metrics = BinaryClassificationMetrics(PredAndLabels)\n",
    "\n",
    "# Area under ROC\n",
    "print(\"Random Forest Area Under ROC\")\n",
    "print(\"Area under ROC = %.2f\" % metrics.areaUnderROC)\n",
    "\n",
    "# Visualization\n",
    "FPR = dict()                                                        # FPR: False Positive Rate\n",
    "tpr = dict()                                                        # TPR: True Positive Rate\n",
    "roc_auc = dict()\n",
    " \n",
    "y_test = [i[1] for i in PredAndLabels_list]\n",
    "y_score = [i[0] for i in PredAndLabels_list]\n",
    " \n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    " \n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Random Forest')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d99843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion Matrix\n",
    "cm_rf_result = predict_test_rf.crosstab(\"prediction\", \"readmitted\")\n",
    "cm_rf_result = cm_rf_result.toPandas()\n",
    "cm_rf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0803af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate Accuracy, Sensitivity, Specificity, Precision for Random Forest Classifier\n",
    "TP = cm_rf_result[\"1\"][0]\n",
    "FP = cm_rf_result[\"0\"][0]\n",
    "TN = cm_rf_result[\"0\"][1]\n",
    "FN = cm_rf_result[\"1\"][1]\n",
    "Accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "Sensitivity = TP/(TP+FN)\n",
    "Specificity = TN/(TN+FP)\n",
    "Precision = TP/(TP+FP)\n",
    "\n",
    "print (\"Accuracy = %0.2f\" %Accuracy )\n",
    "print (\"Sensitivity/Recall = %0.2f\" %Sensitivity )\n",
    "print (\"Specificity = %0.2f\" %Specificity )\n",
    "print (\"Precision = %0.2f\" %Precision )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6764da75",
   "metadata": {},
   "source": [
    "### <font color='skyblue'> 6.1.3: Decision Tree Algorithm</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cbe66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "#Create decision tree model to data train\n",
    "dt=DecisionTreeClassifier(featuresCol = 'Scaled_features', labelCol = 'readmitted', maxDepth = 3)\n",
    "dt_model = dt.fit(train)\n",
    "\n",
    "##Transform model to data test\n",
    "dt_result = dt_model.transform(test)\n",
    "\n",
    "#view id, label, prediction and probability from result of modelling\n",
    "dt_result.select( 'readmitted', 'prediction', 'probability').show(5)\n",
    "\n",
    "# Decision Tree Evaluation\n",
    "#Evaluate model by calculating accuracy and area under curve (AUC)\n",
    "dt_eval = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"readmitted\")\n",
    "dt_eval2= MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"readmitted\")\n",
    "dt_AUC  = dt_eval.evaluate(dt_result)\n",
    "dt_ACC  = dt_eval2.evaluate(dt_result, {dt_eval2.metricName:\"accuracy\"})\n",
    "\n",
    "print(\"Decision Tree Performance Measure\")\n",
    "print(\"Accuracy = %0.2f\" % dt_ACC)\n",
    "print(\"AUC = %.2f\" % dt_AUC)\n",
    "\n",
    "#ROC Grafik\n",
    "PredAndLabels           = dt_result.select(\"probability\", \"readmitted\")\n",
    "PredAndLabels_collect   = PredAndLabels.collect()\n",
    "PredAndLabels_list      = [(float(i[0][0]), 1.0-float(i[1])) for i in PredAndLabels_collect]\n",
    "PredAndLabels           = sc.parallelize(PredAndLabels_list)\n",
    "\n",
    "metrics = BinaryClassificationMetrics(PredAndLabels)\n",
    "\n",
    "# Area under ROC\n",
    "print(\"Decision Tree Area Under ROC\")\n",
    "print(\"Area under ROC = %.2f\" % metrics.areaUnderROC)\n",
    "\n",
    "# Visualization\n",
    "FPR = dict()                                                        # FPR: False Positive Rate\n",
    "tpr = dict()                                                        # TPR: True Positive Rate\n",
    "roc_auc = dict()\n",
    " \n",
    "y_test = [i[1] for i in PredAndLabels_list]\n",
    "y_score = [i[0] for i in PredAndLabels_list]\n",
    " \n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    " \n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Decision Tree')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#confusion matrix\n",
    "cm_dt_result = dt_result.crosstab(\"prediction\", \"readmitted\")\n",
    "cm_dt_result = cm_dt_result.toPandas()\n",
    "cm_dt_result\n",
    "\n",
    "#calculate accuracy, sensitivity, specificity and precision\n",
    "TP = cm_dt_result[\"1\"][0]\n",
    "FP = cm_dt_result[\"0\"][0]\n",
    "TN = cm_dt_result[\"0\"][1]\n",
    "FN = cm_dt_result[\"1\"][1]\n",
    "Accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "Sensitivity = TP/(TP+FN)\n",
    "Specificity = TN/(TN+FP)\n",
    "Precision = TP/(TP+FP)\n",
    "\n",
    "print (\"Accuracy = %0.2f\" %Accuracy )\n",
    "print (\"Sensitivity = %0.2f\" %Sensitivity )\n",
    "print (\"Specificity = %0.2f\" %Specificity )\n",
    "print (\"Precision = %0.2f\" %Precision )\n",
    "\n",
    "#Calculate Gini Coeffiecient from AUC\n",
    "AUC = dt_AUC\n",
    "Gini_dt = (2 * AUC - 1)\n",
    "print(\"AUC=%.2f\" % AUC)\n",
    "print(\"GINI ~=%.2f\" % Gini_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c14ae4",
   "metadata": {},
   "source": [
    "### <font color='skyblue'> 6.1.4:Gradient Boosting Algorithm</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd578c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting\n",
    "#create gradient boosting model in data train\n",
    "gbt = GBTClassifier(featuresCol=\"Scaled_features\", labelCol=\"readmitted\",  maxIter=10)\n",
    "gbt_model = gbt.fit(train)\n",
    "\n",
    "#transfrom model to data test\n",
    "gbt_result = gbt_model.transform(test)\n",
    "\n",
    "#view id, label, prediction and probability from result of modelling\n",
    "gbt_result.select( 'readmitted', 'prediction', 'probability').show(5)\n",
    "\n",
    "#Gradient Boosting Evaluation\n",
    "#Evaluate model by calculating accuracy and area under curve (AUC)\n",
    "gbt_eval = BinaryClassificationEvaluator(rawPredictionCol=\"probability\",labelCol=\"readmitted\")\n",
    "gbt_eval2= MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"readmitted\")\n",
    "gbt_AUC  = gbt_eval.evaluate(gbt_result)\n",
    "gbt_ACC  = gbt_eval2.evaluate(gbt_result, {gbt_eval2.metricName:\"accuracy\"})\n",
    "\n",
    "print(\"Gradient Boosted Tree Performance Measure\")\n",
    "print(\"Accuracy = %0.2f\" % gbt_ACC)\n",
    "print(\"AUC = %.2f\" % gbt_AUC)\n",
    "\n",
    "#ROC Grafik\n",
    "PredAndLabels           = gbt_result.select(\"probability\", \"readmitted\")\n",
    "PredAndLabels_collect   = PredAndLabels.collect()\n",
    "PredAndLabels_list      = [(float(i[0][0]), 1.0-float(i[1])) for i in PredAndLabels_collect]\n",
    "PredAndLabels           = sc.parallelize(PredAndLabels_list)\n",
    "\n",
    "metrics = BinaryClassificationMetrics(PredAndLabels)\n",
    "\n",
    "# Area under ROC\n",
    "print(\"Gradient Boosting Area Under ROC\")\n",
    "print(\"Area under ROC = %.2f\" % metrics.areaUnderROC)\n",
    "\n",
    "# Visualization\n",
    "FPR = dict()                                                        # FPR: False Positive Rate\n",
    "tpr = dict()                                                        # TPR: True Positive Rate\n",
    "roc_auc = dict()\n",
    " \n",
    "y_test = [i[1] for i in PredAndLabels_list]\n",
    "y_score = [i[0] for i in PredAndLabels_list]\n",
    " \n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    " \n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Gradient Boosting')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#Confusion Matrix\n",
    "cm_gbt_result = gbt_result.crosstab(\"prediction\", \"readmitted\")\n",
    "cm_gbt_result = cm_gbt_result.toPandas()\n",
    "cm_gbt_result\n",
    "\n",
    "#calculate accuracy, sensitivity, specificity and precision\n",
    "TP = cm_gbt_result[\"1\"][0]\n",
    "FP = cm_gbt_result[\"0\"][0]\n",
    "TN = cm_gbt_result[\"0\"][1]\n",
    "FN = cm_gbt_result[\"1\"][1]\n",
    "Accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "Sensitivity = TP/(TP+FN)\n",
    "Specificity = TN/(TN+FP)\n",
    "Precision = TP/(TP+FP)\n",
    "\n",
    "print (\"Accuracy = %0.2f\" %Accuracy )\n",
    "print (\"Sensitivity = %0.2f\" %Sensitivity )\n",
    "print (\"Specificity = %0.2f\" %Specificity )\n",
    "print (\"Precision = %0.2f\" %Precision )\n",
    "\n",
    "#Calculate Gini Coefficient from AUC\n",
    "AUC = gbt_AUC\n",
    "Gini_gbt= (2 * AUC -1)\n",
    "\n",
    "print(\"AUC=%.2f\" % AUC)\n",
    "print(\"GINI ~=%.2f\" % Gini_gbt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d1819",
   "metadata": {},
   "source": [
    "## <font color='grey'> 6.2: Select algorithm(s) in a logical manner</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa84607",
   "metadata": {},
   "source": [
    "## <font color='grey'> 6.3: Model Selection</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f351eea3",
   "metadata": {},
   "source": [
    "#  <font color='orange'>Step 7: Data Mining </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b9e3b0",
   "metadata": {},
   "source": [
    "## <font color='grey'> 7.1: Create Logical Test</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bdf869",
   "metadata": {},
   "source": [
    "## <font color='grey'> 7.2: Conducting Data Mining (Model running!)</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d520c7c0",
   "metadata": {},
   "source": [
    "## <font color='grey'> 7.3: Searching for Patterns</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a49d20",
   "metadata": {},
   "source": [
    "# <font color='orange'>Step 8: Interpretation</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f79a3a",
   "metadata": {},
   "source": [
    "## <font color='grey'> 8.1: Study and discuss the mined patterns</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3292811b",
   "metadata": {},
   "source": [
    "## <font color='grey'> 8.2: Visualize the data, results, models and patterns</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9100e978",
   "metadata": {},
   "source": [
    "## <font color='grey'> 8.3: Interpret the data, results, models and patterns</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe38fed5",
   "metadata": {},
   "source": [
    "## <font color='grey'> 8.4: Assess and evaluate the data, results, models and patterns</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c570ec5c",
   "metadata": {},
   "source": [
    "## <font color='grey'> 8.5: Multiple Iterations</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c36b18",
   "metadata": {},
   "source": [
    "### <font color='skyblue'> 8.5.1:1st Iteration</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64075adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = (combined_df.select('Scaled_features','readmitted')).randomSplit([0.80, 0.20], seed = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting\n",
    "#create gradient boosting model in data train\n",
    "gbt = GBTClassifier(featuresCol=\"Scaled_features\", labelCol=\"readmitted\",  maxIter=10)\n",
    "gbt_model = gbt.fit(train)\n",
    "\n",
    "#transfrom model to data test\n",
    "gbt_result = gbt_model.transform(test)\n",
    "\n",
    "#view id, label, prediction and probability from result of modelling\n",
    "gbt_result.select( 'readmitted', 'prediction', 'probability').show(5)\n",
    "\n",
    "#Gradient Boosting Evaluation\n",
    "#Evaluate model by calculating accuracy and area under curve (AUC)\n",
    "gbt_eval = BinaryClassificationEvaluator(rawPredictionCol=\"probability\",labelCol=\"readmitted\")\n",
    "gbt_eval2= MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"readmitted\")\n",
    "gbt_AUC  = gbt_eval.evaluate(gbt_result)\n",
    "gbt_ACC  = gbt_eval2.evaluate(gbt_result, {gbt_eval2.metricName:\"accuracy\"})\n",
    "\n",
    "print(\"Gradient Boosted Tree Performance Measure\")\n",
    "print(\"Accuracy = %0.2f\" % gbt_ACC)\n",
    "print(\"AUC = %.2f\" % gbt_AUC)\n",
    "\n",
    "#ROC Grafik\n",
    "PredAndLabels           = gbt_result.select(\"probability\", \"readmitted\")\n",
    "PredAndLabels_collect   = PredAndLabels.collect()\n",
    "PredAndLabels_list      = [(float(i[0][0]), 1.0-float(i[1])) for i in PredAndLabels_collect]\n",
    "PredAndLabels           = sc.parallelize(PredAndLabels_list)\n",
    "\n",
    "metrics = BinaryClassificationMetrics(PredAndLabels)\n",
    "\n",
    "# Area under ROC\n",
    "print(\"Gradient Boosting Area Under ROC\")\n",
    "print(\"Area under ROC = %.2f\" % metrics.areaUnderROC)\n",
    "\n",
    "# Visualization\n",
    "FPR = dict()                                                        # FPR: False Positive Rate\n",
    "tpr = dict()                                                        # TPR: True Positive Rate\n",
    "roc_auc = dict()\n",
    " \n",
    "y_test = [i[1] for i in PredAndLabels_list]\n",
    "y_score = [i[0] for i in PredAndLabels_list]\n",
    " \n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    " \n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Gradient Boosting')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#Confusion Matrix\n",
    "cm_gbt_result = gbt_result.crosstab(\"prediction\", \"readmitted\")\n",
    "cm_gbt_result = cm_gbt_result.toPandas()\n",
    "cm_gbt_result\n",
    "\n",
    "#calculate accuracy, sensitivity, specificity and precision\n",
    "TP = cm_gbt_result[\"1\"][0]\n",
    "FP = cm_gbt_result[\"0\"][0]\n",
    "TN = cm_gbt_result[\"0\"][1]\n",
    "FN = cm_gbt_result[\"1\"][1]\n",
    "Accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "Sensitivity = TP/(TP+FN)\n",
    "Specificity = TN/(TN+FP)\n",
    "Precision = TP/(TP+FP)\n",
    "\n",
    "print (\"Accuracy = %0.2f\" %Accuracy )\n",
    "print (\"Sensitivity = %0.2f\" %Sensitivity )\n",
    "print (\"Specificity = %0.2f\" %Specificity )\n",
    "print (\"Precision = %0.2f\" %Precision )\n",
    "\n",
    "#Calculate Gini Coefficient from AUC\n",
    "AUC = gbt_AUC\n",
    "Gini_gbt= (2 * AUC -1)\n",
    "\n",
    "print(\"AUC=%.2f\" % AUC)\n",
    "print(\"GINI ~=%.2f\" % Gini_gbt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e199b",
   "metadata": {},
   "source": [
    "### <font color='skyblue'> 8.5.2: Second Iteration</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229994ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = (combined_df.select('Scaled_features','readmitted')).randomSplit([0.85, 0.15], seed = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d3120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting\n",
    "#create gradient boosting model in data train\n",
    "gbt = GBTClassifier(featuresCol=\"Scaled_features\", labelCol=\"readmitted\",  maxIter=10)\n",
    "gbt_model = gbt.fit(train)\n",
    "\n",
    "#transfrom model to data test\n",
    "gbt_result = gbt_model.transform(test)\n",
    "\n",
    "#view id, label, prediction and probability from result of modelling\n",
    "gbt_result.select( 'readmitted', 'prediction', 'probability').show(5)\n",
    "\n",
    "#Gradient Boosting Evaluation\n",
    "#Evaluate model by calculating accuracy and area under curve (AUC)\n",
    "gbt_eval = BinaryClassificationEvaluator(rawPredictionCol=\"probability\",labelCol=\"readmitted\")\n",
    "gbt_eval2= MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"readmitted\")\n",
    "gbt_AUC  = gbt_eval.evaluate(gbt_result)\n",
    "gbt_ACC  = gbt_eval2.evaluate(gbt_result, {gbt_eval2.metricName:\"accuracy\"})\n",
    "\n",
    "print(\"Gradient Boosted Tree Performance Measure\")\n",
    "print(\"Accuracy = %0.2f\" % gbt_ACC)\n",
    "print(\"AUC = %.2f\" % gbt_AUC)\n",
    "\n",
    "#ROC Grafik\n",
    "PredAndLabels           = gbt_result.select(\"probability\", \"readmitted\")\n",
    "PredAndLabels_collect   = PredAndLabels.collect()\n",
    "PredAndLabels_list      = [(float(i[0][0]), 1.0-float(i[1])) for i in PredAndLabels_collect]\n",
    "PredAndLabels           = sc.parallelize(PredAndLabels_list)\n",
    "\n",
    "metrics = BinaryClassificationMetrics(PredAndLabels)\n",
    "\n",
    "# Area under ROC\n",
    "print(\"Gradient Boosting Area Under ROC\")\n",
    "print(\"Area under ROC = %.2f\" % metrics.areaUnderROC)\n",
    "\n",
    "# Visualization\n",
    "FPR = dict()                                                        # FPR: False Positive Rate\n",
    "tpr = dict()                                                        # TPR: True Positive Rate\n",
    "roc_auc = dict()\n",
    " \n",
    "y_test = [i[1] for i in PredAndLabels_list]\n",
    "y_score = [i[0] for i in PredAndLabels_list]\n",
    " \n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    " \n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Gradient Boosting')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#Confusion Matrix\n",
    "cm_gbt_result = gbt_result.crosstab(\"prediction\", \"readmitted\")\n",
    "cm_gbt_result = cm_gbt_result.toPandas()\n",
    "cm_gbt_result\n",
    "\n",
    "#calculate accuracy, sensitivity, specificity and precision\n",
    "TP = cm_gbt_result[\"1\"][0]\n",
    "FP = cm_gbt_result[\"0\"][0]\n",
    "TN = cm_gbt_result[\"0\"][1]\n",
    "FN = cm_gbt_result[\"1\"][1]\n",
    "Accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "Sensitivity = TP/(TP+FN)\n",
    "Specificity = TN/(TN+FP)\n",
    "Precision = TP/(TP+FP)\n",
    "\n",
    "print (\"Accuracy = %0.2f\" %Accuracy )\n",
    "print (\"Sensitivity = %0.2f\" %Sensitivity )\n",
    "print (\"Specificity = %0.2f\" %Specificity )\n",
    "print (\"Precision = %0.2f\" %Precision )\n",
    "\n",
    "#Calculate Gini Coefficient from AUC\n",
    "AUC = gbt_AUC\n",
    "Gini_gbt= (2 * AUC -1)\n",
    "\n",
    "print(\"AUC=%.2f\" % AUC)\n",
    "print(\"GINI ~=%.2f\" % Gini_gbt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c0d3b7",
   "metadata": {},
   "source": [
    "### <font color='skyblue'> 8.5.3:Third Iteration</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9041ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting With Hyper-Parameter\n",
    "#define gradient boosting model\n",
    "gbt_hyper= GBTClassifier(featuresCol=\"Scaled_features\", labelCol=\"readmitted\",maxIter=10, maxDepth=12)\n",
    "\n",
    "# Hyper-Parameter Tuning\n",
    "# paramGrid_gbt = ParamGridBuilder() \\\n",
    "#     .addGrid(gbt_hyper.maxIter, [10])\\\n",
    "#     .addGrid(gbt_hyper.maxDepth, [6, 7,10]) \\\n",
    "#     .build()\n",
    "# crossval_gbt = CrossValidator(estimator=gbt_hyper,\n",
    "#                              estimatorParamMaps=paramGrid_gbt,\n",
    "#                              evaluator=BinaryClassificationEvaluator(),\n",
    "#                              numFolds=3)\n",
    "#fit model to data train\n",
    "gbt_model_hyper = gbt_hyper.fit(train)\n",
    "\n",
    "#transfrom model to data test\n",
    "gbt_result_hyper = gbt_model_hyper.transform(test)\n",
    "\n",
    "#view id, label, prediction and probability from result of modelling\n",
    "gbt_result_hyper.select( 'readmitted', 'prediction', 'probability').show(5)\n",
    "\n",
    "#Gradient Boosting With Hyper-Parameter Evaluation\n",
    "#Evaluate model by calculating accuracy and area under curve (AUC)\n",
    "gbt_eval_hyper = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"readmitted\")\n",
    "gbt_eval_hyper2= MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"readmitted\")\n",
    "gbt_hyper_AUC  = gbt_eval_hyper.evaluate(gbt_result_hyper)\n",
    "gbt_hyper_ACC  = gbt_eval_hyper2.evaluate(gbt_result_hyper, {gbt_eval_hyper2.metricName:\"accuracy\"})\n",
    "\n",
    "\n",
    "print(\"Gradient Boosted Tree Performance Measure\")\n",
    "print(\"Accuracy = %0.2f\" % gbt_hyper_ACC)\n",
    "print(\"AUC = %.2f\" % gbt_hyper_AUC)\n",
    "\n",
    "#ROC Grafik\n",
    "PredAndLabels           = gbt_result_hyper.select(\"probability\", \"readmitted\")\n",
    "PredAndLabels_collect   = PredAndLabels.collect()\n",
    "PredAndLabels_list      = [(float(i[0][0]), 1.0-float(i[1])) for i in PredAndLabels_collect]\n",
    "PredAndLabels           = sc.parallelize(PredAndLabels_list)\n",
    "\n",
    "metrics = BinaryClassificationMetrics(PredAndLabels)\n",
    "\n",
    "# Area under ROC\n",
    "print(\"Gradient Boosting Area Under ROC\")\n",
    "print(\"Area under ROC = %.2f\" % metrics.areaUnderROC)\n",
    "\n",
    "# Visualization\n",
    "FPR = dict()                                                        # FPR: False Positive Rate\n",
    "tpr = dict()                                                        # TPR: True Positive Rate\n",
    "roc_auc = dict()\n",
    " \n",
    "y_test = [i[1] for i in PredAndLabels_list]\n",
    "y_score = [i[0] for i in PredAndLabels_list]\n",
    " \n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    " \n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Gradient Boosting')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#confusion Matrix\n",
    "cm_gbt_result_hyper = gbt_result_hyper.crosstab(\"prediction\", \"readmitted\")\n",
    "cm_gbt_result_hyper = cm_gbt_result_hyper.toPandas()\n",
    "cm_gbt_result_hyper\n",
    "\n",
    "#calculate accuracy, sensitivity, specificity and precision\n",
    "TP = cm_gbt_result_hyper[\"1\"][0]\n",
    "FP = cm_gbt_result_hyper[\"0\"][0]\n",
    "TN = cm_gbt_result_hyper[\"0\"][1]\n",
    "FN = cm_gbt_result_hyper[\"1\"][1]\n",
    "Accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "Sensitivity = TP/(TP+FN)\n",
    "Specificity = TN/(TN+FP)\n",
    "Precision = TP/(TP+FP)\n",
    "\n",
    "print (\"Accuracy = %0.2f\" %Accuracy )\n",
    "print (\"Sensitivity = %0.2f\" %Sensitivity )\n",
    "print (\"Specificity = %0.2f\" %Specificity )\n",
    "print (\"Precision = %0.2f\" %Precision )\n",
    "\n",
    "#Calculate Gini Coefficient from AUC\n",
    "AUC = gbt_hyper_AUC\n",
    "Gini_gbt_hyper= (2 * AUC -1)\n",
    "\n",
    "print(\"AUC=%.2f\" % AUC)\n",
    "print(\"GINI ~=%.2f\" % Gini_gbt_hyper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d7f42",
   "metadata": {},
   "source": [
    "### <font color='skyblue'> 8.5.4: Fourth Iteration</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4aaada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting With Hyper-Parameter\n",
    "#define gradient boosting model\n",
    "gbt_hyper= GBTClassifier(featuresCol=\"Scaled_features\", labelCol=\"readmitted\",maxIter=10, maxDepth=10)\n",
    "\n",
    "# Hyper-Parameter Tuning\n",
    "# paramGrid_gbt = ParamGridBuilder() \\\n",
    "#     .addGrid(gbt_hyper.maxIter, [10])\\\n",
    "#     .addGrid(gbt_hyper.maxDepth, [6, 7,10]) \\\n",
    "#     .build()\n",
    "# crossval_gbt = CrossValidator(estimator=gbt_hyper,\n",
    "#                              estimatorParamMaps=paramGrid_gbt,\n",
    "#                              evaluator=BinaryClassificationEvaluator(),\n",
    "#                              numFolds=3)\n",
    "#fit model to data train\n",
    "gbt_model_hyper = gbt_hyper.fit(train)\n",
    "\n",
    "#transfrom model to data test\n",
    "gbt_result_hyper = gbt_model_hyper.transform(test)\n",
    "\n",
    "#view id, label, prediction and probability from result of modelling\n",
    "gbt_result_hyper.select( 'readmitted', 'prediction', 'probability').show(5)\n",
    "\n",
    "#Gradient Boosting With Hyper-Parameter Evaluation\n",
    "#Evaluate model by calculating accuracy and area under curve (AUC)\n",
    "gbt_eval_hyper = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"readmitted\")\n",
    "gbt_eval_hyper2= MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"readmitted\")\n",
    "gbt_hyper_AUC  = gbt_eval_hyper.evaluate(gbt_result_hyper)\n",
    "gbt_hyper_ACC  = gbt_eval_hyper2.evaluate(gbt_result_hyper, {gbt_eval_hyper2.metricName:\"accuracy\"})\n",
    "\n",
    "\n",
    "print(\"Gradient Boosted Tree Performance Measure\")\n",
    "print(\"Accuracy = %0.2f\" % gbt_hyper_ACC)\n",
    "print(\"AUC = %.2f\" % gbt_hyper_AUC)\n",
    "\n",
    "#ROC Grafik\n",
    "PredAndLabels           = gbt_result_hyper.select(\"probability\", \"readmitted\")\n",
    "PredAndLabels_collect   = PredAndLabels.collect()\n",
    "PredAndLabels_list      = [(float(i[0][0]), 1.0-float(i[1])) for i in PredAndLabels_collect]\n",
    "PredAndLabels           = sc.parallelize(PredAndLabels_list)\n",
    "\n",
    "metrics = BinaryClassificationMetrics(PredAndLabels)\n",
    "\n",
    "# Area under ROC\n",
    "print(\"Gradient Boosting Area Under ROC\")\n",
    "print(\"Area under ROC = %.2f\" % metrics.areaUnderROC)\n",
    "\n",
    "# Visualization\n",
    "FPR = dict()                                                        # FPR: False Positive Rate\n",
    "tpr = dict()                                                        # TPR: True Positive Rate\n",
    "roc_auc = dict()\n",
    " \n",
    "y_test = [i[1] for i in PredAndLabels_list]\n",
    "y_score = [i[0] for i in PredAndLabels_list]\n",
    " \n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    " \n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Gradient Boosting')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#confusion Matrix\n",
    "cm_gbt_result_hyper = gbt_result_hyper.crosstab(\"prediction\", \"readmitted\")\n",
    "cm_gbt_result_hyper = cm_gbt_result_hyper.toPandas()\n",
    "cm_gbt_result_hyper\n",
    "\n",
    "#calculate accuracy, sensitivity, specificity and precision\n",
    "TP = cm_gbt_result_hyper[\"1\"][0]\n",
    "FP = cm_gbt_result_hyper[\"0\"][0]\n",
    "TN = cm_gbt_result_hyper[\"0\"][1]\n",
    "FN = cm_gbt_result_hyper[\"1\"][1]\n",
    "Accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "Sensitivity = TP/(TP+FN)\n",
    "Specificity = TN/(TN+FP)\n",
    "Precision = TP/(TP+FP)\n",
    "\n",
    "print (\"Accuracy = %0.2f\" %Accuracy )\n",
    "print (\"Sensitivity = %0.2f\" %Sensitivity )\n",
    "print (\"Specificity = %0.2f\" %Specificity )\n",
    "print (\"Precision = %0.2f\" %Precision )\n",
    "\n",
    "#Calculate Gini Coefficient from AUC\n",
    "AUC = gbt_hyper_AUC\n",
    "Gini_gbt_hyper= (2 * AUC -1)\n",
    "\n",
    "print(\"AUC=%.2f\" % AUC)\n",
    "print(\"GINI ~=%.2f\" % Gini_gbt_hyper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b0360",
   "metadata": {},
   "source": [
    "### <font color='skyblue'> 8.5.5 : Fifth Iteration</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74738667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting With Hyper-Parameter\n",
    "#define gradient boosting model\n",
    "gbt_hyper= GBTClassifier(featuresCol=\"Scaled_features\", labelCol=\"readmitted\",maxIter=10, maxDepth=6)\n",
    "\n",
    "# Hyper-Parameter Tuning\n",
    "# paramGrid_gbt = ParamGridBuilder() \\\n",
    "#     .addGrid(gbt_hyper.maxIter, [10])\\\n",
    "#     .addGrid(gbt_hyper.maxDepth, [6, 7,10]) \\\n",
    "#     .build()\n",
    "# crossval_gbt = CrossValidator(estimator=gbt_hyper,\n",
    "#                              estimatorParamMaps=paramGrid_gbt,\n",
    "#                              evaluator=BinaryClassificationEvaluator(),\n",
    "#                              numFolds=3)\n",
    "#fit model to data train\n",
    "gbt_model_hyper = gbt_hyper.fit(train)\n",
    "\n",
    "#transfrom model to data test\n",
    "gbt_result_hyper = gbt_model_hyper.transform(test)\n",
    "\n",
    "#view id, label, prediction and probability from result of modelling\n",
    "gbt_result_hyper.select( 'readmitted', 'prediction', 'probability').show(5)\n",
    "\n",
    "#Gradient Boosting With Hyper-Parameter Evaluation\n",
    "#Evaluate model by calculating accuracy and area under curve (AUC)\n",
    "gbt_eval_hyper = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"readmitted\")\n",
    "gbt_eval_hyper2= MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"readmitted\")\n",
    "gbt_hyper_AUC  = gbt_eval_hyper.evaluate(gbt_result_hyper)\n",
    "gbt_hyper_ACC  = gbt_eval_hyper2.evaluate(gbt_result_hyper, {gbt_eval_hyper2.metricName:\"accuracy\"})\n",
    "\n",
    "\n",
    "print(\"Gradient Boosted Tree Performance Measure\")\n",
    "print(\"Accuracy = %0.2f\" % gbt_hyper_ACC)\n",
    "print(\"AUC = %.2f\" % gbt_hyper_AUC)\n",
    "\n",
    "#ROC Grafik\n",
    "PredAndLabels           = gbt_result_hyper.select(\"probability\", \"readmitted\")\n",
    "PredAndLabels_collect   = PredAndLabels.collect()\n",
    "PredAndLabels_list      = [(float(i[0][0]), 1.0-float(i[1])) for i in PredAndLabels_collect]\n",
    "PredAndLabels           = sc.parallelize(PredAndLabels_list)\n",
    "\n",
    "metrics = BinaryClassificationMetrics(PredAndLabels)\n",
    "\n",
    "# Area under ROC\n",
    "print(\"Gradient Boosting Area Under ROC\")\n",
    "print(\"Area under ROC = %.2f\" % metrics.areaUnderROC)\n",
    "\n",
    "# Visualization\n",
    "FPR = dict()                                                        # FPR: False Positive Rate\n",
    "tpr = dict()                                                        # TPR: True Positive Rate\n",
    "roc_auc = dict()\n",
    " \n",
    "y_test = [i[1] for i in PredAndLabels_list]\n",
    "y_score = [i[0] for i in PredAndLabels_list]\n",
    " \n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    " \n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Gradient Boosting')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#confusion Matrix\n",
    "cm_gbt_result_hyper = gbt_result_hyper.crosstab(\"prediction\", \"readmitted\")\n",
    "cm_gbt_result_hyper = cm_gbt_result_hyper.toPandas()\n",
    "cm_gbt_result_hyper\n",
    "\n",
    "#calculate accuracy, sensitivity, specificity and precision\n",
    "TP = cm_gbt_result_hyper[\"1\"][0]\n",
    "FP = cm_gbt_result_hyper[\"0\"][0]\n",
    "TN = cm_gbt_result_hyper[\"0\"][1]\n",
    "FN = cm_gbt_result_hyper[\"1\"][1]\n",
    "Accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "Sensitivity = TP/(TP+FN)\n",
    "Specificity = TN/(TN+FP)\n",
    "Precision = TP/(TP+FP)\n",
    "\n",
    "print (\"Accuracy = %0.2f\" %Accuracy )\n",
    "print (\"Sensitivity = %0.2f\" %Sensitivity )\n",
    "print (\"Specificity = %0.2f\" %Specificity )\n",
    "print (\"Precision = %0.2f\" %Precision )\n",
    "\n",
    "#Calculate Gini Coefficient from AUC\n",
    "AUC = gbt_hyper_AUC\n",
    "Gini_gbt_hyper= (2 * AUC -1)\n",
    "\n",
    "print(\"AUC=%.2f\" % AUC)\n",
    "print(\"GINI ~=%.2f\" % Gini_gbt_hyper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbced6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f48cc94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5c140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1bb43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e36dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22706183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276c15a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29e2c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da642d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563ef2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f22762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bb4e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede70ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
